
#================================================================
# Check, install, and load required libraries
#================================================================
required_packages <- c("rjags", "dplyr", "tidyr", "pROC", "coda", "runjags", "TSA")

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)  
    library(pkg, character.only = TRUE)         
  } else {
    library(pkg, character.only = TRUE)         
  }
}
#================================================================


#================================================================
# Load required functions for GFN classification operation 
#================================================================

## Addition ##
GFN.add <- function(A, B) {
  mean <- A[1]+B[1] 
  variance <- A[2]+B[2] 
  return(c(mean, variance))
}

## Subtraction ##
GFN.sub <- function(A, B) {
  mean <- A[1]-B[1]
  variance <- A[2]+B[2] 
  return(c(mean, variance))
}

## Multiplication ##
GFN.multi <- function(A, B) {
  mean <- A[1]*B[1]
  variance <- (B[2]*A[1]^2)+(A[2]*B[1]^2)+(B[2]*A[2])
  return(c(mean, variance))
}

## Division ##
GFN.div <- function(A, B) {
  mean <- A[1]*((1/B[1])+(B[2]/B[1]^3))
  variance <- (A[1]^2*(1/B[1]^4)*B[2])+((1/B[1]^2)*A[2])-(A[2]*(1/B[1]^4)*B[2])
  return(c(mean, variance))
}

# New exp() function
GFN.exp <- function(A) {
  mean <- exp(A[1]*A[2]) 
  variance <- ((exp(A[2])-1)*(exp((2*A[1])+(A[2]))))
  return(c(mean, variance))
}

## Reciprocal ##
GFN.reci <- function(A) {
  mean <- 1/A[1] 
  variance <- (1/A[1]^4)*A[2] 
  return(c(mean, variance))
}

GFN.multi.crisp_GFN <- function(A, B) {
  mean <- A[1]*B 
  variance <- B^2 * A[2]^2 
  return(c(mean, variance))
}

GFN.add.crisp_GFN <- function(A, B) {
  mean <- A[1]+B 
  variance <- A[2] 
  return(c(mean, variance))
}

z.score_classify <- function(probabilities, Tau) {
  classifications <- numeric(nrow(probabilities))
  for (i in 1:nrow(probabilities)) {
    mean_Tau <- Tau[1]
    sd_Tau <- max(Tau[2], 0.0001)
    mean_prob.a <- probabilities[i, 1]
    z_prob.a <- (mean_prob.a - mean_Tau) / sd_Tau
    if (z_prob.a >= 0) {
      classifications[i] <- 1
    } else {
      classifications[i] <- 0
    }
  }
  return(classifications)
}
#================================================================


#================================================================
# Load Main doFBLR function
#================================================================

doTS_FBLR <- function(data, freq = 12, burnIn = 500, adapt = 500, numChain = 2, thinning = 5, SavedSteps = 1000, 
                   priorMeanZbeta = 0, lambda = 2, RV_name = "Class", 
                   min_tau1 = 0.001, max_tau1 = 0.999, min_tau2 = 0.001, max_tau2 = 0.999) {
  
  # Data prep
  y <- data[[RV_name]]
  x <- data %>% select(-all_of(RV_name))
  
  # Remove 'time' and 'season' from standardization process
  exclude_vars <- "time" # GC3: this ensures time is not scaled
  x_to_scale <- x %>% select(-any_of(exclude_vars))
  x_not_scaled <- x %>% select(any_of(exclude_vars))

  # Standardize the data
  zx_scaled <- scale(x_to_scale)
  zx <- cbind(zx_scaled, x_not_scaled)
  
  # Harmonic analysis
  data1 <- ts(zx, frequency = freq)
  har. <- harmonic(data1, 1)
  cosEffect <- har.[,1]
  sinEffect <- har.[,2]
  
  # GC3: from 27/02 meet - scale the harmonics
  sinEffect <- as.numeric(scale(sinEffect))
  cosEffect <- as.numeric(scale(cosEffect))
  
  # GC3: input the harmonics in zx so they work as features
  zx <- cbind(zx, sinEffect, cosEffect)
  
  # GC3: convert the time feature into continuous
  time_element <- time(data1)
  zx[, "time"] <- time_element
  zx <- zx[, c(setdiff(names(zx), "time"), "time")] #GC3: ensures time is last column
  
  # GC3: we can also try scaling time (comment out the below line to not scale time)
  zx[, "time"] <- scale(zx[, "time"])
                        
  # GC3: zx must be matrix for GFN operations to work
  zx <- as.matrix(zx)
  
  # GC3: healthcheck for finite values in zx and y
  if (any(!is.finite(y))) stop("All y values must be finite.")
  if (any(!is.finite(zx))) stop("All x values must be finite.")

  # HD1: Why not scale()?
  # zzx <- scale(x)
  # zx - zzx
  # HD1: no need to standardise time and season. Is there any specific reason for the?
  # HD1: Also you got harmnoic before standardisation.
  # GC2: good points, thanks :) I moved harmonic analysis to use zx after standardization.
  # GC2: also, I used a scale() function and removed season and time from standardization process.
  # HD2: All good.
  # GC3: thanks :)
  
  # HD1: You got cosEffect and sinEffect above. Why don't you use them and calculate seasonEffect below?
  # HD1: seasonEffect is the same as cosEffect.
  # HD1: sum(seasonEffect - cosEffect) = 1.509164e-13
  # GC2: my bad, makes sense. I did it twice for no reason and the seasonEffect was wrong.
  # GC2: what if we input sinEffect and cosEffect into the data list directly and use them that way?
  # GC2: see changes below.
  # HD2: Yep, it is best to input sinEffect and cosEffect into the data list directly and use them that way.
  # GC3: thanks :)
  
  
  # JAGS data
  x_mean <- colMeans(x)  # Mean of each predictor
  x_sd <- apply(x, 2, sd)  # SD of each predictor
    x_sd[x_sd == 0] <- 1
  
  # GC3: removed cosEffect and sinEffect, since these are now scaled features within zx
  # GC3: also removed xm and xsd, since we do not do back-transformation any more
  dataList <- list(
    zx = zx,
    y = y,
    Nx = ncol(zx),
    Ntotal = nrow(zx),
    lambda = lambda
  )
  
  
  # Notes of 13/02 meeting:
  #*: calculate these sin and cos outside of JAGS for greater efficiency, then input as variables.
  #*: also take the k/6 part outside at the bottom of JAGS.
  #*: install TSA package and use ts function to create seasonal time series. 
  #*: Then run harmonic function to find cos and sin, then input into JAGS, instead of calcuating within.
  #*: Use acf function to find the right frequency to use.
  #*GC1: I did all of these changes, removing the calcs from JABS. Efficiency improves by 26%!
  
  # JAGS model
  modelString <- "
    # Model
    model {

        for (i in 1:Ntotal) {
            # Logistic regression with seasonality
            # HD1: Here your model is missing sin component of harmonic model.
            # HD1: seasonEffect does not have a coefficient in the model.
            # HD1: The model should have cosEffect and sinEffect and they shouldbe multiplied by coefficients like the other features.
            # GC2: so inputing cosEffect and sinEffect instead of seasonEffect, we can then do the m[i] below this way?
            # HD2: Yes, that's right.
            # GC3: Cheers :)
            
            # HD2: Why do you have season as a fetuure in your model? Seasonality is already captured by harmonic coefficients. 
            # GC3: Then, we can drop. It was selected as part of feature selection, but we can use your logic here on the paper and remove.
            # GC3: The 'season' feature has been removed.
            
            # GC3: from 27/02 meeting, since cos and sin effects are now scaled features within zx, we remove from model
            m[i] <- guess * 0.5 + (1 - guess) * ilogit(
                zbeta0 
                + sum(zbeta[1:Nx] * zx[i, 1:Nx]) 
            )
            y[i] ~ dbern(m[i])
        }

        # Priors
        guess ~ dbeta(1, 4) 
        # HD2: You can play with the parameters of dbeta() to see if it helps with large betas. 
        # HD2: As you increase the mean of this prior, more instances will be coming from a random guess.
        # GC3: from 27/02 meeting - we keep as (1,4) but can try to make the 2nd value even smaller.
        
        
        zbeta0 ~ dnorm(0, 0.25)
        
        for (j in 1:Nx) {
            # Laplace prior for L1 regularization
            zbeta[j] ~ ddexp(0, lambda)
        }

        # Hyperprior for lambda
        lambda ~ dgamma(0.1, 0.1)
        
        # GC3: since cos and sin effects are scaled features within zx, we do not need to call their priors

        # Priors for harmonic coefficients
        # HD1: Your model does not have sin and cos coefficients. They must appear in the model.
        # GC2: I added them above. Is it correct way?
        # HD2: Yep.
        # GC3: Cheers :)
        #zsinBeta ~ dnorm(0, 0.25)
        #zcosBeta ~ dnorm(0, 0.25)
        
        
        # GC3: since cos and sin effects are scaled features within zx, we do not need to back-transform. We will work with zx and zbeta instead.
        
        # Transform to original scale:
        # GC2: is this what you mean on 255-262 comments? add it on here?
        # HD2: Yep.
        # GC3: Cheers :)
        #for (j in 1:(Nx-1)) {
        #        beta[j] <- zbeta[j] / xsd[j]
        #        }
        #beta[Nx] <- zbeta[Nx] #GC3: from 27/02 meeting - added this to remove back transformation of time
        #beta0 <- zbeta0 - sum(zbeta[1:Nx] * xm[1:Nx] / xsd[1:Nx])
        
    }
"
  writeLines(modelString, con = "TEMPmodel.txt")
  
  # Run JAGS model
  parameters <- c("zbeta0", "zbeta") # GC3: we only work with zbetas now, as per FBLR process too
  
  # HD2: You need to send the number of MCMC samples
  
  numSavedSteps = SavedSteps # HD2: This can be another argument of the function / GC3: added as function input
  numChain = numChain # HD2: No need to have 4 chains for such a simple model / #GC3: added 2 in function input
  thinning = thinning # HD2: 7 may be unnecessarily too much. Since 5 works, we save time. / #GC3: added 5 in function input
  runJagsOut <- run.jags(
    model = "TEMPmodel.txt",
    monitor = parameters,
    data = dataList,
    n.chains = numChain,
    adapt = adapt,
    burnin = burnIn,
    thin = thinning,
    sample=numSavedSteps , # HD2: Added / #GC3: thanks
    summarise = FALSE,
    plots = FALSE
  )
  codaSamples <- as.mcmc.list(runJagsOut)
  
  # Summary stats
  model_summary <- summary(runJagsOut)
  #print(model_summary) 
  
  # HD1: Added
  # GC2: temporarily commented out as it was giving errors. I need to fix. 
  Nx <- dim(zx)[2]
  diagMCMC( codaSamples , parName="zbeta0" )
  for ( i in 1:Nx){ # HD1: +2 are harmonic coefficients.# HD2: Modified this as you have different names for harmonic coefficients.
   diagMCMC( codaSamples , parName=paste0("zbeta[",i,"]") )
  }
  # GC3: since sin and cos effects are zbetas now, the below 2 lines not needed
  #diagMCMC( codaSamples , parName="sinBeta" )
  #diagMCMC( codaSamples , parName="cosBeta" )
  graphics.off()
  # HD1: Added
  
  # Dynamically determine indices for beta parameters
  start_index <- which(rownames(model_summary) == "zbeta0")
  if (length(start_index) == 0) stop("Start index for 'zbeta0' not found in model summary.")
  end_index <- start_index + Nx 
  
  # Extract beta statistics dynamically
  # HD1: These are not posterior estimates of beta parameters. They are zbetas.
  # GC2: Yep, mistake on my part! Did not include the additional parameters before running the JAGS... fixed it now.
  
  # HD1: JAGS model does not convert zbetas to betas. 
  # HD1: You need to do this conversion in the JAGS model text. 
  # HD1: And then get the posterior estimates of beta parameters here.
  # GC2: I added the code to transform the betas within JAGS (lines 211-216).
  # GC2: But if we look at the SD values for some of these 'betas', some are super large, which cause NaN/Inf values on GFN.probabilities
  # GC3: from 27/02 meet - we will work with zbetas and zx instead, which is already scaled
  
  #print(model_summary)
  
  # GC2: Extract median and SD of beta values. This way, we include sinBeta and cosBeta as GFNs.
  beta_indices <- grep("^zbeta", rownames(model_summary)) 
  beta_means <- model_summary[beta_indices, "Median"]
  beta_sd <- model_summary[beta_indices, "SD"]

  # Store the coefficients in a 2-column matrix
  gaussian_fuzzy_matrix <- matrix(NA, nrow = length(beta_means), ncol = 2)
  for (i in 1:length(beta_means)) {
    gaussian_fuzzy_matrix[i, ] <- c(beta_means[i], beta_sd[i])
  }
  print(gaussian_fuzzy_matrix)
  # GC3: coefficient values are much smaller, great!
  

  ## GFN Logistic Function -- GFN/Crisp Operations Method ##
  n <- nrow(zx)
  Np <- ncol(zx)
  predy <- matrix(NA, nrow = n, ncol = 2)
  b0 <- gaussian_fuzzy_matrix[1,]
  b <- gaussian_fuzzy_matrix[-1,]
  
  for (i in 1:n) {  
    bx <- b0
    for(j in 1:Np) {
      bx <- GFN.add(bx, GFN.multi.crisp_GFN(b[j,],zx[i,j])) 
    }
    predy[i,] <- GFN.reci(GFN.add.crisp_GFN(GFN.exp(GFN.multi.crisp_GFN(bx,-1)),1)) 
  }
  GFN.probabilities <- predy 
  print(GFN.probabilities)
  
  # Health-check for normal values on GFN probabilities (non-NA)
  if (any(is.na(GFN.probabilities))) {
    stop("Error: GFN.probabilities contains NA values!")
  }
  
  # GC3: but still getting NaN / Inf values for GFN.probabilities
  # GC3: maybe we should try to scale time too ....
  # GC3: well this is funny..... I scaled "time" (see line 134) and getting perfect results!!! Works amazing.... :D
 
  
  ## Classification with Tau optimization to chase desired performance outcome ##
  # Function for performance measures
  compute_performance <- function(gfn_tau, epsilon = 0.01) {
    gaussian.pred.bin <- z.score_classify(GFN.probabilities, gfn_tau)
    
    tp <- sum(gaussian.pred.bin == 1 & y == 1) + epsilon  # Add epsilon to avoid zero
    tn <- sum(gaussian.pred.bin == 0 & y == 0) + epsilon
    fp <- sum(gaussian.pred.bin == 1 & y == 0) + epsilon
    fn <- sum(gaussian.pred.bin == 0 & y == 1) + epsilon
    
    sensitivity <- round(tp / (tp + fn), 3)
    specificity <- round(tn / (tn + fp), 3)
    precision <- round(tp / (tp + fp), 3)
    recall <- sensitivity
    f1_score <- round(2 * (precision * recall) / (precision + recall), 3)
    mcc <- round(((tp * tn) - (fp * fn)) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)), 3)
    
    list(sensitivity = sensitivity, specificity = specificity, f1_score = f1_score, mcc = mcc)
  }
  
  
  # Df to store results
  results_df <- data.frame(
    Tau_Mean = numeric(),
    Tau_Variance = numeric(),
    Sensitivity = numeric(),
    Specificity = numeric(),
    F1_Score = numeric(),
    MCC = numeric(),
    TP = numeric(),
    TN = numeric(),
    FP = numeric(),
    FN = numeric()
  )
  
  # Grid search
  row_index <- 1
  for (tau1 in seq(min_tau1, max_tau1, by = 0.05)) {
    for (tau2 in seq(min_tau2, max_tau2, by = 0.05)) {
      gfn_tau <- c(tau1, tau2)
      gaussian.pred.bin <- z.score_classify(GFN.probabilities, gfn_tau)  
      
      # Compute performance measures
      scores <- compute_performance(gfn_tau)  
      results_df[row_index, "Tau_Mean"] <- tau1
      results_df[row_index, "Tau_Variance"] <- tau2
      results_df[row_index, c("Sensitivity", "Specificity", "F1_Score", "MCC")] <- unlist(scores)
      
      # Compute confusion matrix
      fblr3.conf_matrix <- table(Actual = y, Predicted = gaussian.pred.bin)
      results_df[row_index, c("TP", "TN", "FP", "FN")] <- c(fblr3.conf_matrix[2, 2], fblr3.conf_matrix[1, 1], 
                                                            fblr3.conf_matrix[1, 2], fblr3.conf_matrix[2, 1])
      
      # Compute ROC/AUC
      roc_data <- roc(y, gaussian.pred.bin)
      results_df[row_index, "AUC"] <- auc(roc_data)
      row_index <- row_index + 1 
    }
  }
  
  # Optimized performance output using AUC/ROC
  max_auc_index <- which.max(results_df$AUC)
  best_results <- results_df[max_auc_index, ]
  print(best_results)
  
  
  # Plot ROC curve
  gaussian.pred.bin.best <- z.score_classify(GFN.probabilities, c(best_results[,1],best_results[,2]))
  plot.roc(y, gaussian.pred.bin.best, main = "ROC Curve", col = "blue")
  
  
  # Print all results
  write.csv(results_df, "performance_results.csv") 
  write.csv(gaussian.pred.bin.best, "predictions.csv") 
  write.csv(GFN.probabilities, "probabilities.csv") 
  
  beta_means_raw <- model_summary[1:nrow(gaussian_fuzzy_matrix), "Median"]
  beta_sd_raw <- model_summary[1:nrow(gaussian_fuzzy_matrix), "SD"]
  gaussian_fuzzy_matrix_raw <- matrix(NA, nrow = length(beta_means_raw), ncol = 2)
  for (i in 1:length(beta_means_raw)) {
    gaussian_fuzzy_matrix_raw[i, ] <- c(beta_means_raw[i], beta_sd_raw[i])
  }
  write.csv(gaussian_fuzzy_matrix_raw, "coefficients.csv") 
  
  # Return the results
  return(list(
    coefficients = gaussian_fuzzy_matrix,
    model_summary = model_summary,
    samples = codaSamples
  ))
}
#================================================================


# Import data
#setwd("~/Documents/MATH800/George/paper6_bushfires/TS-FBLR/VIC1") # HD1: Added
source("DBDA2E-utilities.R")   # HD1: Added
data <- read.csv("TUR1.csv")

# Check harmonic data and acf to find freq
data1 <- ts(data, frequency = 6)
acf(data1, lag.max=40)
har. <- harmonic(data1, 1)

# Run TS_FBLR
results <- doTS_FBLR(data)


#For benchmarking:
#*: Comparison cannot be against classical ML or other such methods. Instead:
#*: find deep learning models that can handle seasonality to compare against - LSTM or CRM
#*: but first, let's lock in this code, then I will find and run those methods
