
#================================================================
# Check, install, and load required libraries
#================================================================
required_packages <- c("rjags", "dplyr", "tidyr", "pROC", "coda", "runjags", "TSA")

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)  
    library(pkg, character.only = TRUE)         
  } else {
    library(pkg, character.only = TRUE)         
  }
}
#================================================================


#================================================================
# Load required functions for GFN classification operation 
#================================================================

## Addition ##
GFN.add <- function(A, B) {
  mean <- A[1]+B[1] 
  variance <- A[2]+B[2] 
  return(c(mean, variance))
}

## Subtraction ##
GFN.sub <- function(A, B) {
  mean <- A[1]-B[1]
  variance <- A[2]+B[2] 
  return(c(mean, variance))
}

## Multiplication ##
GFN.multi <- function(A, B) {
  mean <- A[1]*B[1]
  variance <- (B[2]*A[1]^2)+(A[2]*B[1]^2)+(B[2]*A[2])
  return(c(mean, variance))
}

## Division ##
GFN.div <- function(A, B) {
  mean <- A[1]*((1/B[1])+(B[2]/B[1]^3))
  variance <- (A[1]^2*(1/B[1]^4)*B[2])+((1/B[1]^2)*A[2])-(A[2]*(1/B[1]^4)*B[2])
  return(c(mean, variance))
}

# New exp() function
GFN.exp <- function(A) {
  mean <- exp(A[1]*A[2]) 
  variance <- ((exp(A[2])-1)*(exp((2*A[1])+(A[2]))))
  return(c(mean, variance))
}

## Reciprocal ##
GFN.reci <- function(A) {
  mean <- 1/A[1] 
  variance <- (1/A[1]^4)*A[2] 
  return(c(mean, variance))
}

GFN.multi.crisp_GFN <- function(A, B) {
  mean <- A[1]*B 
  variance <- B^2 * A[2]^2 
  return(c(mean, variance))
}

GFN.add.crisp_GFN <- function(A, B) {
  mean <- A[1]+B 
  variance <- A[2] 
  return(c(mean, variance))
}

z.score_classify <- function(probabilities, Tau) {
  classifications <- numeric(nrow(probabilities))
  for (i in 1:nrow(probabilities)) {
    mean_Tau <- Tau[1]
    sd_Tau <- max(Tau[2], 0.0001)
    mean_prob.a <- probabilities[i, 1]
    z_prob.a <- (mean_prob.a - mean_Tau) / sd_Tau
    if (z_prob.a >= 0) {
      classifications[i] <- 1
    } else {
      classifications[i] <- 0
    }
  }
  return(classifications)
}
#================================================================


#================================================================
# Load Main doFBLR function
#================================================================

doTS_FBLR <- function(data, freq = 12, burnIn = 500, adapt = 500, numChain = 2, thinning = 5, SavedSteps = 1000, 
                   priorMeanZbeta = 0, lambda = 2, RV_name = "Class", 
                   min_tau1 = 0.001, max_tau1 = 0.999, min_tau2 = 0.001, max_tau2 = 0.999) {
  
  # Data prep
  y <- data[[RV_name]]
  x <- data %>% select(-all_of(RV_name))
  
  # Remove 'time' and 'season' from standardization process
  exclude_vars <- "time" 
  x_to_scale <- x %>% select(-any_of(exclude_vars))
  x_not_scaled <- x %>% select(any_of(exclude_vars))

  # Standardize the data
  zx_scaled <- scale(x_to_scale)
  zx <- cbind(zx_scaled, x_not_scaled)
  
  # Harmonic analysis
  data1 <- ts(zx, frequency = freq)
  har. <- harmonic(data1, 1)
  cosEffect <- har.[,1]
  sinEffect <- har.[,2]
  sinEffect <- as.numeric(scale(sinEffect))
  cosEffect <- as.numeric(scale(cosEffect))

  zx <- cbind(zx, sinEffect, cosEffect)
  time_element <- time(data1)
  zx[, "time"] <- time_element
  zx <- zx[, c(setdiff(names(zx), "time"), "time")] 
  zx[, "time"] <- scale(zx[, "time"])                  
  zx <- as.matrix(zx)
  
  # healthcheck for finite values in zx and y
  if (any(!is.finite(y))) stop("All y values must be finite.")
  if (any(!is.finite(zx))) stop("All x values must be finite.")

  # zzx <- scale(x)
  # zx - zzx
  
  # JAGS data
  x_mean <- colMeans(x)  # Mean of each predictor
  x_sd <- apply(x, 2, sd)  # SD of each predictor
    x_sd[x_sd == 0] <- 1

  dataList <- list(
    zx = zx,
    y = y,
    Nx = ncol(zx),
    Ntotal = nrow(zx),
    lambda = lambda
  )
  
  
  # JAGS model
  modelString <- "
    # Model
    model {

        for (i in 1:Ntotal) {
            # Logistic regression with seasonality
            
            m[i] <- guess * 0.5 + (1 - guess) * ilogit(
                zbeta0 
                + sum(zbeta[1:Nx] * zx[i, 1:Nx]) 
            )
            y[i] ~ dbern(m[i])
        }

        # Priors
        guess ~ dbeta(1, 4) 
   
        zbeta0 ~ dnorm(0, 0.25)
        
        for (j in 1:Nx) {
            # Laplace prior for L1 regularization
            zbeta[j] ~ ddexp(0, lambda)
        }

        # Hyperprior for lambda
        lambda ~ dgamma(0.1, 0.1)
        
        # Priors for harmonic coefficients
        
        #zsinBeta ~ dnorm(0, 0.25)
        #zcosBeta ~ dnorm(0, 0.25)
        
        # Transform to original scale:
       
        #for (j in 1:(Nx-1)) {
        #        beta[j] <- zbeta[j] / xsd[j]
        #        }
        #beta[Nx] <- zbeta[Nx] #GC3: from 27/02 meeting - added this to remove back transformation of time
        #beta0 <- zbeta0 - sum(zbeta[1:Nx] * xm[1:Nx] / xsd[1:Nx])
        
    }
"
  writeLines(modelString, con = "TEMPmodel.txt")
  
  # Run JAGS model
  parameters <- c("zbeta0", "zbeta") # GC3: we only work with zbetas now, as per FBLR process too
  
  numSavedSteps = SavedSteps # HD2: This can be another argument of the function / GC3: added as function input
  numChain = numChain # HD2: No need to have 4 chains for such a simple model / #GC3: added 2 in function input
  thinning = thinning # HD2: 7 may be unnecessarily too much. Since 5 works, we save time. / #GC3: added 5 in function input
  runJagsOut <- run.jags(
    model = "TEMPmodel.txt",
    monitor = parameters,
    data = dataList,
    n.chains = numChain,
    adapt = adapt,
    burnin = burnIn,
    thin = thinning,
    sample=numSavedSteps , # HD2: Added / #GC3: thanks
    summarise = FALSE,
    plots = FALSE
  )
  codaSamples <- as.mcmc.list(runJagsOut)
  
  # Summary stats
  model_summary <- summary(runJagsOut)
  #print(model_summary) 
  
  Nx <- dim(zx)[2]
  diagMCMC( codaSamples , parName="zbeta0" )
  for ( i in 1:Nx){ # HD1: +2 are harmonic coefficients.
   diagMCMC( codaSamples , parName=paste0("zbeta[",i,"]") )
  }

  #diagMCMC( codaSamples , parName="sinBeta" )
  #diagMCMC( codaSamples , parName="cosBeta" )
  graphics.off()
  
  # Dynamically determine indices for beta parameters
  start_index <- which(rownames(model_summary) == "zbeta0")
  if (length(start_index) == 0) stop("Start index for 'zbeta0' not found in model summary.")
  end_index <- start_index + Nx 
  
  # Extract beta statistics dynamically
 
  #print(model_summary)
  
  # Extract median and SD of beta values. This way, we include sinBeta and cosBeta as GFNs.
  beta_indices <- grep("^zbeta", rownames(model_summary)) 
  beta_means <- model_summary[beta_indices, "Median"]
  beta_sd <- model_summary[beta_indices, "SD"]

  # Store the coefficients in a 2-column matrix
  gaussian_fuzzy_matrix <- matrix(NA, nrow = length(beta_means), ncol = 2)
  for (i in 1:length(beta_means)) {
    gaussian_fuzzy_matrix[i, ] <- c(beta_means[i], beta_sd[i])
  }
  print(gaussian_fuzzy_matrix)
  

  ## GFN Logistic Function -- GFN/Crisp Operations Method ##
  n <- nrow(zx)
  Np <- ncol(zx)
  predy <- matrix(NA, nrow = n, ncol = 2)
  b0 <- gaussian_fuzzy_matrix[1,]
  b <- gaussian_fuzzy_matrix[-1,]
  
  for (i in 1:n) {  
    bx <- b0
    for(j in 1:Np) {
      bx <- GFN.add(bx, GFN.multi.crisp_GFN(b[j,],zx[i,j])) 
    }
    predy[i,] <- GFN.reci(GFN.add.crisp_GFN(GFN.exp(GFN.multi.crisp_GFN(bx,-1)),1)) 
  }
  GFN.probabilities <- predy 
  print(GFN.probabilities)
  
  # Health-check for normal values on GFN probabilities (non-NA)
  if (any(is.na(GFN.probabilities))) {
    stop("Error: GFN.probabilities contains NA values!")
  }
  
  ## Classification with Tau optimization to chase desired performance outcome ##
  # Function for performance measures
  compute_performance <- function(gfn_tau, epsilon = 0.01) {
    gaussian.pred.bin <- z.score_classify(GFN.probabilities, gfn_tau)
    
    tp <- sum(gaussian.pred.bin == 1 & y == 1) + epsilon  # Add epsilon to avoid zero
    tn <- sum(gaussian.pred.bin == 0 & y == 0) + epsilon
    fp <- sum(gaussian.pred.bin == 1 & y == 0) + epsilon
    fn <- sum(gaussian.pred.bin == 0 & y == 1) + epsilon
    
    sensitivity <- round(tp / (tp + fn), 3)
    specificity <- round(tn / (tn + fp), 3)
    precision <- round(tp / (tp + fp), 3)
    recall <- sensitivity
    f1_score <- round(2 * (precision * recall) / (precision + recall), 3)
    mcc <- round(((tp * tn) - (fp * fn)) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)), 3)
    
    list(sensitivity = sensitivity, specificity = specificity, f1_score = f1_score, mcc = mcc)
  }
  
  
  # Df to store results
  results_df <- data.frame(
    Tau_Mean = numeric(),
    Tau_Variance = numeric(),
    Sensitivity = numeric(),
    Specificity = numeric(),
    F1_Score = numeric(),
    MCC = numeric(),
    TP = numeric(),
    TN = numeric(),
    FP = numeric(),
    FN = numeric()
  )
  
  # Grid search
  row_index <- 1
  for (tau1 in seq(min_tau1, max_tau1, by = 0.05)) {
    for (tau2 in seq(min_tau2, max_tau2, by = 0.05)) {
      gfn_tau <- c(tau1, tau2)
      gaussian.pred.bin <- z.score_classify(GFN.probabilities, gfn_tau)  
      
      # Compute performance measures
      scores <- compute_performance(gfn_tau)  
      results_df[row_index, "Tau_Mean"] <- tau1
      results_df[row_index, "Tau_Variance"] <- tau2
      results_df[row_index, c("Sensitivity", "Specificity", "F1_Score", "MCC")] <- unlist(scores)
      
      # Compute confusion matrix
      fblr3.conf_matrix <- table(Actual = y, Predicted = gaussian.pred.bin)
      results_df[row_index, c("TP", "TN", "FP", "FN")] <- c(fblr3.conf_matrix[2, 2], fblr3.conf_matrix[1, 1], 
                                                            fblr3.conf_matrix[1, 2], fblr3.conf_matrix[2, 1])
      
      # Compute ROC/AUC
      roc_data <- roc(y, gaussian.pred.bin)
      results_df[row_index, "AUC"] <- auc(roc_data)
      row_index <- row_index + 1 
    }
  }
  
  # Optimized performance output using AUC/ROC
  max_auc_index <- which.max(results_df$AUC)
  best_results <- results_df[max_auc_index, ]
  print(best_results)
  
  
  # Plot ROC curve
  gaussian.pred.bin.best <- z.score_classify(GFN.probabilities, c(best_results[,1],best_results[,2]))
  plot.roc(y, gaussian.pred.bin.best, main = "ROC Curve", col = "blue")
  
  
  # Print all results
  write.csv(results_df, "performance_results.csv") 
  write.csv(gaussian.pred.bin.best, "predictions.csv") 
  write.csv(GFN.probabilities, "probabilities.csv") 
  
  beta_means_raw <- model_summary[1:nrow(gaussian_fuzzy_matrix), "Median"]
  beta_sd_raw <- model_summary[1:nrow(gaussian_fuzzy_matrix), "SD"]
  gaussian_fuzzy_matrix_raw <- matrix(NA, nrow = length(beta_means_raw), ncol = 2)
  for (i in 1:length(beta_means_raw)) {
    gaussian_fuzzy_matrix_raw[i, ] <- c(beta_means_raw[i], beta_sd_raw[i])
  }
  write.csv(gaussian_fuzzy_matrix_raw, "coefficients.csv") 
  
  # Return the results
  return(list(
    coefficients = gaussian_fuzzy_matrix,
    model_summary = model_summary,
    samples = codaSamples
  ))
}
#================================================================


# Import data
source("DBDA2E-utilities.R")   # HD1: Added
data <- read.csv("TUR1.csv")

# Check harmonic data and acf to find freq
data1 <- ts(data, frequency = 6)
acf(data1, lag.max=40)
har. <- harmonic(data1, 1)

# Run TS_FBLR
results <- doTS_FBLR(data)

